# -*- coding: utf-8 -*-
"""Phase 0-Prep the Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OMjhXvEu4JILH_7KMk5KbxiQGxvtlRuG
"""

#!/usr/bin/env python3
"""
parse_initial_solution.py
Parses initialSolution.in into:
 - pairings.csv           (pairing_id, base, legs_as_semicolon_separated)
 - legs.csv               (leg_index, leg_id)
 - incidence.csv          (leg_index, pairing_index)
 - pairing_legs_expanded.csv (pairing_index, pairing_id, leg_index, leg_id)
 - warnings.txt           (any parsing warnings)
Usage:
    python parse_initial_solution.py path/to/initialSolution.in
"""

import sys
import re
import csv
from collections import OrderedDict

def parse_line(line):
    """
    Parses a single line from the initial solution file into its core components: pairing ID, base, and ordered leg list.
    Designed to be tolerant of minor formatting inconsistencies (extra whitespace, missing semicolons, stray characters). 
    If the line cannot be confidently parsed, it is flagged rather than failing hard so downstream parsing can continue. 
    Also collects per-line warnings for suspicious or truncated tokens.
    """
    # Example expected line:
    # Pairing 161 : Base BASE2 : LEG_05_2 , LEG_05_1 , LEG_05_27 , LEG_05_0;
    line = line.strip()
    if not line:
        return None
    # Remove trailing semicolon if present
    if line.endswith(';'):
        line = line[:-1]
    # Regex for pairing id and base
    # split by ':' to be robust to formatting variations
    parts = [p.strip() for p in line.split(':')]
    # Expect at least 3 parts: "Pairing X", "Base Y", "LEG..., LEG..."
    if len(parts) < 3:
        return ('WARN', line)
    # pairing id from parts[0]
    m = re.search(r'Pairing\s+(\d+)', parts[0], re.IGNORECASE)
    if not m:
        return ('WARN', line)
    pairing_id = int(m.group(1))
    # base from parts[1]
    m2 = re.search(r'Base\s+(.+)', parts[1], re.IGNORECASE)
    if not m2:
        base = parts[1]
    else:
        base = m2.group(1).strip()
    # remaining parts after second colon may contain additional colons; join them
    legs_part = ':'.join(parts[2:]).strip()
    # split legs by commas
    raw_tokens = [tok.strip() for tok in legs_part.split(',')]
    # clean tokens: remove empty, remove trailing semicolons (already removed), keep TDH_ prefix
    legs = []
    warnings = []
    for tok in raw_tokens:
        if not tok:
            continue
        # remove stray trailing characters
        tok_clean = tok.strip()
        # if token contains illegal/truncated marker like '$' flag it
        if '$' in tok_clean or tok_clean.endswith(',') or tok_clean.endswith(';'):
            warnings.append(f"Truncated or suspicious token '{tok}' in pairing {pairing_id}")
        # some tokens in your paste had stray trailing characters like '$' or ended abruptly
        legs.append(tok_clean)
    return ('OK', pairing_id, base, legs, warnings)

def main(path):

    """
    Entry point for parsing the full initial solution file and materializing multiple CSV views of the same data.
    Reads and validates pairings, assigns stable zero-based indices to both pairings and legs, and emits:

    - pairing-level table,
    - unique leg index table,
    - sparse incidence mapping (leg ↔ pairing),
    - fully expanded pairing–leg table.

    Warnings are aggregated across the entire file and written separately so imperfect input does not silently corrupt the outputs.
    """
    pairings = []  # list of (pairing_id, base, legs)
    warnings = []
    with open(path, 'r', encoding='utf-8') as f:
        for raw in f:
            r = parse_line(raw)
            if r is None:
                continue
            if r[0] == 'WARN':
                warnings.append(f"Unparseable line: {r[1]}")
            else:
                _, pid, base, legs, w = r
                pairings.append((pid, base, legs))
                warnings.extend(w)
    # sort pairings by pairing id for deterministic order
    pairings.sort(key=lambda x: x[0])
    # build leg dictionary (ordered)
    leg_to_index = OrderedDict()
    for i, (pid, base, legs) in enumerate(pairings):
        for leg in legs:
            if leg not in leg_to_index:
                leg_to_index[leg] = len(leg_to_index)  # zero-based index

    # write pairings.csv
    with open('pairings.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['pairing_index','pairing_id','base','legs_semicolon'])
        for p_index, (pid, base, legs) in enumerate(pairings):
            writer.writerow([p_index, pid, base, ';'.join(legs)])

    # write legs.csv
    with open('legs.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['leg_index','leg_id'])
        for leg_id, idx in leg_to_index.items():
            writer.writerow([idx, leg_id])

    # write incidence.csv (sparse triples)
    with open('incidence.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['leg_index','pairing_index'])  # both zero-based
        for p_index, (pid, base, legs) in enumerate(pairings):
            for leg in legs:
                lidx = leg_to_index[leg]
                writer.writerow([lidx, p_index])

    # write pairing_legs_expanded.csv
    with open('pairing_legs_expanded.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['pairing_index','pairing_id','leg_index','leg_id'])
        for p_index, (pid, base, legs) in enumerate(pairings):
            for leg in legs:
                writer.writerow([p_index, pid, leg_to_index[leg], leg])

    # warnings
    if warnings:
        with open('warnings.txt', 'w', encoding='utf-8') as f:
            for w in warnings:
                f.write(w + '\n')
        print(f"Parsing completed with {len(warnings)} warnings (see warnings.txt).")
    else:
        print("Parsing completed with no warnings.")

    print(f"Produced files: pairings.csv, legs.csv, incidence.csv, pairing_legs_expanded.csv")

if __name__ == '__main__':


    path = "/content/sample_data/initialSolution.in"
    main(path)
