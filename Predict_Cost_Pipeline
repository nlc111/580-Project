# -*- coding: utf-8 -*-
"""Predict_Cost_Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13xU-EMRjfpbZzFwXzQofBkNJvaJVAdV6
"""



"""
Input data:

day_*.csv

initialSolution.txt

solution_*.txt


"""



#!/usr/bin/env python3
"""
pairing_cost_predictor.py
Predicts crew pairing costs using Ridge regression with polynomial features.
"""

from pathlib import Path
import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
from sklearn.linear_model import Ridge
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import re
import warnings
warnings.filterwarnings('ignore')

# -------------------------------
# 1. Parse leg data
# -------------------------------

def parse_leg_files(day_files_folder):
    """
    Robustly load day_*.csv files with flexible date/time parsing.
    Handles extra spaces, multiple delimiters, and cleans column names.

    Returns:
        dict: leg_id -> {
            dep_airport,
            arr_airport,
            dep_dt,
            arr_dt,
            duration_min
        }
    """
    leg_dict = {}
    files_loaded = 0
    error_rows = 0

    REQUIRED_COLS = {
        "leg_nb", "airport_dep", "date_dep", "hour_dep",
        "airport_arr", "date_arr", "hour_arr"
    }

    print(f"Loading day files from: {day_files_folder}")

    for day_num in range(1, 32):
        filepath = os.path.join(day_files_folder, f"day_{day_num}.csv")
        if not os.path.exists(filepath):
            print(f"  Warning: {filepath} not found, skipping.")
            continue

        try:
            # Auto-detect delimiter (tab or comma)
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline()
            delimiter = '\t' if '\t' in first_line else ','

            df = pd.read_csv(
                filepath,
                sep=delimiter,
                skipinitialspace=True,
                dtype=str
            )

            # Normalize column names
            df.columns = [c.strip().lstrip("#").lower() for c in df.columns]

            missing = REQUIRED_COLS - set(df.columns)
            if missing:
                raise ValueError(f"{filepath} missing required columns: {missing}")

            for _, row in df.iterrows():
                try:
                    # Flexible datetime parsing
                    dep_str = f"{row['date_dep'].strip()} {row['hour_dep'].strip()}"
                    arr_str = f"{row['date_arr'].strip()} {row['hour_arr'].strip()}"
                    dep_dt = pd.to_datetime(dep_str, errors='coerce', dayfirst=False)
                    arr_dt = pd.to_datetime(arr_str, errors='coerce', dayfirst=False)

                    if pd.isna(dep_dt) or pd.isna(arr_dt):
                        raise ValueError(f"Could not parse dep/arr datetime: {dep_str} / {arr_str}")

                    # Handle overnight flights
                    if arr_dt < dep_dt:
                        arr_dt += pd.Timedelta(days=1)

                    duration_min = (arr_dt - dep_dt).total_seconds() / 60

                    leg_id = row["leg_nb"].strip()
                    leg_dict[leg_id] = {
                        "dep_airport": row["airport_dep"].strip(),
                        "arr_airport": row["airport_arr"].strip(),
                        "dep_dt": dep_dt,
                        "arr_dt": arr_dt,
                        "duration_min": duration_min,
                    }

                except Exception as e:
                    error_rows += 1
                    print(f"  Bad row in {filepath}, leg={row.get('leg_nb','?')}: {e}")

            files_loaded += 1

        except Exception as e:
            print(f"  Error reading {filepath}: {e}")

    print(f"Loaded {files_loaded} day files | {len(leg_dict)} legs | {error_rows} bad rows")

    if not leg_dict:
        raise RuntimeError("No leg data loaded — check day files.")

    return leg_dict


# -------------------------------
# 2. Parse schedule data
# -------------------------------

def parse_schedules_from_file(schedules_file):
    schedule_dict = {}
    if not os.path.exists(schedules_file):
        return schedule_dict
    with open(schedules_file, 'r') as f:
        content = f.read()
    pattern = r'schedule\s+(\d+)\s+(\w+)\s+\((\w+)\)\s*:\s*([^;]+)'
    matches = re.findall(pattern, content)
    for schedule_num, emp_id, base, activities in matches:
        schedule_id = f"schedule_{schedule_num}"
        activity_list = [act.strip() for act in activities.split('--->')]
        schedule_dict[schedule_id] = {
            'schedule_num': int(schedule_num),
            'employee': emp_id,
            'base': base,
            'activities': activity_list,
            'n_activities': len(activity_list),
            'n_vacations': sum(1 for act in activity_list if 'VACATION' in act)
        }
    return schedule_dict

def load_credited_hours(file_path):
    schedules = {}
    if not os.path.exists(file_path):
        return schedules
    with open(file_path, "r") as f:
        content = f.read()
    pattern = re.compile(
        r"Schedule (\d+)\(BASE(\d+)\) :\s*"
        r"--------> credited hours\s*:\s*([\d.]+)\s*"
        r"--------> schedule cost\s*:\s*([\d.]+)\s*"
        r"--------> number of vacations\s*:\s*(\d+)",
        re.MULTILINE
    )
    for match in pattern.finditer(content):
        schedule_id = f"schedule_{match.group(1)}"
        schedules[schedule_id] = {
            "base": int(match.group(2)),
            "credited_hours": float(match.group(3)),
            "schedule_cost": float(match.group(4)),
            "vacations": int(match.group(5))
        }
    return schedules

def parse_pairings_from_file(pairings_file, schedule_dict, solution_id=0):
    """
    Parses initialSolution.txt and returns a list of pairing dicts.

    Expected format (loosely):
      Pairing X (BASEY): LEG_1 ---> LEG_2 ---> ...

    Returns:
        list of pairings
    """
    if not os.path.exists(pairings_file):
        raise FileNotFoundError(f"Pairings file not found: {pairings_file}")

    print(f"Loading pairings from: {pairings_file}")

    pairings = []

    with open(pairings_file, 'r') as f:
        content = f.read()

    # Pairing pattern
    pairing_pattern = re.compile(
        r'Pairing\s+(\d+)\s*\((BASE\d+)\)\s*:\s*([^\n]+)'
    )

    matches = pairing_pattern.findall(content)

    for pairing_id, base, duties_str in matches:
        duties = [d.strip() for d in duties_str.split('--->')]

        # Try to infer schedule by matching overlap with schedule activities
        matched_schedule = None
        duty_set = set(duties)

        for sched_id, sched in schedule_dict.items():
            sched_acts = set(
                a for a in sched['activities']
                if a.startswith('LEG_') or a.startswith('PAL_') or a.startswith('TDH_')
            )
            if duty_set.issubset(sched_acts):
                matched_schedule = sched_id
                break

        pairings.append({
            'id': int(pairing_id),
            'base': base,
            'duties': duties,
            'schedule': matched_schedule,
            'solution_id': solution_id
        })

    print(f"Loaded {len(pairings)} pairings")
    return pairings


def attach_schedule_info_to_pairings(pairings, credited_dict):
    for pairing in pairings:
        schedule_id = pairing.get('schedule')
        if schedule_id and schedule_id in credited_dict:
            pairing['schedule_cost'] = credited_dict[schedule_id]['schedule_cost']
            pairing['schedule_hours'] = credited_dict[schedule_id]['credited_hours']
            pairing['schedule_vacations'] = credited_dict[schedule_id]['vacations']
        else:
            pairing['schedule_cost'] = 0.0
            pairing['schedule_hours'] = 0.0
            pairing['schedule_vacations'] = 0
    return pairings


# Check if pairings are linked to schedules
linked_count = sum(1 for p in pairings if p.get('schedule') is not None)
print(f"\nPairings linked to schedules: {linked_count} / {len(pairings)}")

# Check if any pairing has a cost
has_cost = sum(1 for p in pairings if p.get('cost', 0) > 0)
print(f"Pairings with cost > 0: {has_cost} / {len(pairings)}")

if has_cost == 0:
    print("\n⚠️ PROBLEM: No pairings have costs assigned!")
    # Check one pairing
    if len(pairings) > 0:
        p = pairings[0]
        print(f"\nExample pairing 1:")
        print(f"  schedule: {p.get('schedule')}")
        print(f"  cost: {p.get('cost', 0)}")
        print(f"  schedule_cost: {p.get('schedule_cost', 0)}")

# -------------------------------
# 3. Pairing features
# -------------------------------

def pairing_to_features(pairing, leg_dict, schedule_dict):
    duties = pairing.get('duties', [])
    valid_duties = []
    for d in duties:
        clean_id = d.replace('PAL_LEG_', 'LEG_').replace('TDH_LEG_', 'LEG_').replace('TDH_AGR_', 'LEG_')
        if clean_id in leg_dict:
            valid_duties.append(clean_id)
    if not valid_duties:
        return {f: 0 for f in [
            'n_duties','n_deadheads','total_duration','max_duration','min_duration',
            'avg_gap','max_gap','multi_day_flag','first_hour','last_hour',
            'base','schedule_hours','schedule_cost','schedule_vacations'
        ]}
    durations = [leg_dict[d]['duration_min'] for d in valid_duties]
    n_duties = len(valid_duties)
    n_deadheads = sum(1 for d in duties if d.startswith('PAL_') or d.startswith('TDH_'))
    total_duration = sum(durations)
    max_duration = max(durations)
    min_duration = min(durations)
    sorted_duties = sorted([leg_dict[d] for d in valid_duties], key=lambda x: x['dep_dt'])
    gaps = [(sorted_duties[i+1]['dep_dt'] - sorted_duties[i]['arr_dt']).total_seconds()/60
            for i in range(len(sorted_duties)-1)]
    gaps = [max(0,g) for g in gaps]
    avg_gap = np.mean(gaps) if gaps else 0
    max_gap = np.max(gaps) if gaps else 0
    multi_day_flag = 1 if len(set(d['dep_dt'].date() for d in sorted_duties)) > 1 else 0
    first_hour = sorted_duties[0]['dep_dt'].hour
    last_hour = sorted_duties[-1]['arr_dt'].hour
    schedule_id = pairing.get('schedule')
    schedule_info = schedule_dict.get(schedule_id, {}) if schedule_id else {}
    features = {
        'n_duties': n_duties,
        'n_deadheads': n_deadheads,
        'total_duration': total_duration,
        'max_duration': max_duration,
        'min_duration': min_duration,
        'avg_gap': avg_gap,
        'max_gap': max_gap,
        'multi_day_flag': multi_day_flag,
        'first_hour': first_hour,
        'last_hour': last_hour,
        'base': pairing.get('base',0),
        'schedule_hours': schedule_info.get('n_activities',0),
        'schedule_cost': pairing.get('schedule_cost',0),
        'schedule_vacations': schedule_info.get('n_vacations',0)
    }
    return features

def build_feature_dataframe(pairings, leg_dict, schedule_dict):
    feature_rows = []
    targets = []
    groups = []
    for pairing in pairings:
        feat = pairing_to_features(pairing, leg_dict, schedule_dict)
        feature_rows.append(feat)
        targets.append(pairing.get('cost',0))
        groups.append(pairing.get('solution_id',0))
    X = pd.DataFrame(feature_rows)
    y = np.array(targets)
    groups = np.array(groups)
    print(f"Built feature matrix: {X.shape[0]} samples, {X.shape[1]} features")
    return X, y, groups

# -------------------------------
# 4. Train and predict
# -------------------------------

def train_and_cv(X, y, groups, degree=2, alpha=1.0):
    numeric_cols = ['n_duties','n_deadheads','total_duration','max_duration','min_duration',
                    'avg_gap','max_gap','first_hour','last_hour','schedule_hours',
                    'schedule_cost','schedule_vacations']
    categorical_cols = ['base','multi_day_flag']
    poly_transformer = PolynomialFeatures(degree=degree, include_bias=False)
    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    numeric_data_poly = poly_transformer.fit_transform(X[numeric_cols].values)
    categorical_data = ohe.fit_transform(X[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])
    logo = LeaveOneGroupOut()
    n_splits = logo.get_n_splits(X_final, y, groups)
    if n_splits > 1:
        for train_idx, test_idx in logo.split(X_final, y, groups):
            X_train, X_test = X_final[train_idx], X_final[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            model = Ridge(alpha=alpha)
            model.fit(X_train, y_train)
    final_model = Ridge(alpha=alpha)
    final_model.fit(X_final, y)
    return final_model, poly_transformer, ohe, numeric_cols, categorical_cols

def predict_costs(model, poly_transformer, ohe, numeric_cols, categorical_cols,
                  pairings, leg_dict, schedule_dict):
    X_new = pd.DataFrame([pairing_to_features(p, leg_dict, schedule_dict) for p in pairings])
    numeric_data_poly = poly_transformer.transform(X_new[numeric_cols].values)
    categorical_data = ohe.transform(X_new[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])
    y_pred = model.predict(X_final)
    for p, pred in zip(pairings, y_pred):
        p['pred_cost'] = pred
    return pairings



# -------------------------------
# 5. Main
# -------------------------------

def main(day_files_folder="/content/sample_data",
         schedules_file="/content/sample_data/solution_0.txt",
         pairings_file="/content/sample_data/initialSolution.txt",
         credited_hours_file="/content/sample_data/creditedHours.txt"):

    print("="*60)
    print("Crew Pairing Cost Prediction System")
    print("="*60)

    leg_dict = parse_leg_files(day_files_folder)
    schedule_dict = parse_schedules_from_file(schedules_file)
    credited_dict = load_credited_hours(credited_hours_file)

    # Attach credited hours / schedule_cost to schedules and pairings
    attach_schedule_info_to_pairings([{'schedule':k} for k in schedule_dict.keys()], credited_dict)

    # Load pairings
    pairings = parse_pairings_from_file(pairings_file, schedule_dict)
    attach_schedule_info_to_pairings(pairings, credited_dict)

    X, y, groups = build_feature_dataframe(pairings, leg_dict, schedule_dict)
    model, poly, ohe, num_cols, cat_cols = train_and_cv(X, y, groups)
    pairings_with_pred = predict_costs(model, poly, ohe, num_cols, cat_cols, pairings, leg_dict, schedule_dict)
    return {'model': model, 'pairings': pairings_with_pred, 'leg_dict': leg_dict, 'schedule_dict': schedule_dict}

if __name__ == "__main__":
    results = main(
        day_files_folder="/content/sample_data/instance1/",
        schedules_file="/content/sample_data/solution_0.txt",
        pairings_file="/content/sample_data/initialSolution.txt",
        credited_hours_file="/content/sample_data/creditedHours.txt"
    )



#!/usr/bin/env python3
"""
pairing_cost_predictor.py
Predicts crew pairing costs using Ridge regression with polynomial features.
"""

from pathlib import Path
import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
from sklearn.linear_model import Ridge
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import re
import warnings
warnings.filterwarnings('ignore')

# -------------------------------
# 1. Parse leg data
# -------------------------------

def parse_leg_files(day_files_folder):
    """
    Robustly load day_*.csv files with flexible date/time parsing.
    Handles extra spaces, multiple delimiters, and cleans column names.

    Returns:
        dict: leg_id -> {
            dep_airport,
            arr_airport,
            dep_dt,
            arr_dt,
            duration_min
        }
    """
    leg_dict = {}
    files_loaded = 0
    error_rows = 0

    REQUIRED_COLS = {
        "leg_nb", "airport_dep", "date_dep", "hour_dep",
        "airport_arr", "date_arr", "hour_arr"
    }

    print(f"Loading day files from: {day_files_folder}")

    for day_num in range(1, 32):
        filepath = os.path.join(day_files_folder, f"day_{day_num}.csv")
        if not os.path.exists(filepath):
            print(f"  Warning: {filepath} not found, skipping.")
            continue

        try:
            # Auto-detect delimiter (tab or comma)
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline()
            delimiter = '\t' if '\t' in first_line else ','

            df = pd.read_csv(
                filepath,
                sep=delimiter,
                skipinitialspace=True,
                dtype=str
            )

            # Normalize column names
            df.columns = [c.strip().lstrip("#").lower() for c in df.columns]

            missing = REQUIRED_COLS - set(df.columns)
            if missing:
                raise ValueError(f"{filepath} missing required columns: {missing}")

            for _, row in df.iterrows():
                try:
                    # Flexible datetime parsing
                    dep_str = f"{row['date_dep'].strip()} {row['hour_dep'].strip()}"
                    arr_str = f"{row['date_arr'].strip()} {row['hour_arr'].strip()}"
                    dep_dt = pd.to_datetime(dep_str, errors='coerce', dayfirst=False)
                    arr_dt = pd.to_datetime(arr_str, errors='coerce', dayfirst=False)

                    if pd.isna(dep_dt) or pd.isna(arr_dt):
                        raise ValueError(f"Could not parse dep/arr datetime: {dep_str} / {arr_str}")

                    # Handle overnight flights
                    if arr_dt < dep_dt:
                        arr_dt += pd.Timedelta(days=1)

                    duration_min = (arr_dt - dep_dt).total_seconds() / 60

                    leg_id = row["leg_nb"].strip()
                    leg_dict[leg_id] = {
                        "dep_airport": row["airport_dep"].strip(),
                        "arr_airport": row["airport_arr"].strip(),
                        "dep_dt": dep_dt,
                        "arr_dt": arr_dt,
                        "duration_min": duration_min,
                    }

                except Exception as e:
                    error_rows += 1
                    print(f"  Bad row in {filepath}, leg={row.get('leg_nb','?')}: {e}")

            files_loaded += 1

        except Exception as e:
            print(f"  Error reading {filepath}: {e}")

    print(f"Loaded {files_loaded} day files | {len(leg_dict)} legs | {error_rows} bad rows")

    if not leg_dict:
        raise RuntimeError("No leg data loaded — check day files.")

    return leg_dict


# -------------------------------
# 2. Parse schedule data
# -------------------------------

def parse_schedules_from_file(schedules_file):
    schedule_dict = {}
    if not os.path.exists(schedules_file):
        return schedule_dict
    with open(schedules_file, 'r') as f:
        content = f.read()
    pattern = r'schedule\s+(\d+)\s+(\w+)\s+\((\w+)\)\s*:\s*([^;]+)'
    matches = re.findall(pattern, content)
    for schedule_num, emp_id, base, activities in matches:
        schedule_id = f"schedule_{schedule_num}"
        activity_list = [act.strip() for act in activities.split('--->')]
        schedule_dict[schedule_id] = {
            'schedule_num': int(schedule_num),
            'employee': emp_id,
            'base': base,
            'activities': activity_list,
            'n_activities': len(activity_list),
            'n_vacations': sum(1 for act in activity_list if 'VACATION' in act)
        }
    return schedule_dict

def load_credited_hours(file_path):
    schedules = {}
    if not os.path.exists(file_path):
        return schedules
    with open(file_path, "r") as f:
        content = f.read()
    pattern = re.compile(
        r"Schedule (\d+)\(BASE(\d+)\) :\s*"
        r"--------> credited hours\s*:\s*([\d.]+)\s*"
        r"--------> schedule cost\s*:\s*([\d.]+)\s*"
        r"--------> number of vacations\s*:\s*(\d+)",
        re.MULTILINE
    )
    for match in pattern.finditer(content):
        schedule_id = f"schedule_{match.group(1)}"
        schedules[schedule_id] = {
            "base": int(match.group(2)),
            "credited_hours": float(match.group(3)),
            "schedule_cost": float(match.group(4)),
            "vacations": int(match.group(5))
        }
    return schedules

def parse_pairings_from_file(pairings_file, schedule_dict, solution_id=0):
    """
    Parses initialSolution.txt and returns a list of pairing dicts.
    """
    if not os.path.exists(pairings_file):
        raise FileNotFoundError(f"Pairings file not found: {pairings_file}")

    print(f"Loading pairings from: {pairings_file}")

    pairings = []

    with open(pairings_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # Debug: show first 500 characters
    print(f"File preview (first 500 chars):\n{content[:500]}\n")

    # Try the pattern with colons and semicolons
    pairing_pattern = re.compile(
        r'Pairing\s+(\d+)\s*:\s*Base\s+(\w+)\s*:\s*([^;]+);',
        re.MULTILINE
    )

    matches = pairing_pattern.findall(content)
    print(f"Pattern matched {len(matches)} pairings")

    if len(matches) == 0:
        print("ERROR: No matches found!")
        print("Trying alternate pattern...")
        # Try with arrows instead
        alt_pattern = re.compile(r'Pairing\s+(\d+)\s*\((\w+)\)\s*:\s*(.+?)(?=Pairing|\Z)', re.DOTALL)
        alt_matches = alt_pattern.findall(content)
        print(f"Alternate pattern matched {len(alt_matches)} pairings")
        if len(alt_matches) > 0:
            print("Use the alternate pattern!")
        return []

    for pairing_id, base, duties_str in matches:
        # Split by comma
        duties = [d.strip() for d in duties_str.split(',')]
        duties = [d for d in duties if d]  # Remove empty

        pairings.append({
            'id': int(pairing_id),
            'base': base,
            'duties': duties,
            'schedule': None,
            'solution_id': solution_id
        })

    print(f"Loaded {len(pairings)} pairings")
    return pairings


def attach_schedule_info_to_pairings(pairings, credited_dict):
    for pairing in pairings:
        schedule_id = pairing.get('schedule')
        if schedule_id and schedule_id in credited_dict:
            pairing['schedule_cost'] = credited_dict[schedule_id]['schedule_cost']
            pairing['schedule_hours'] = credited_dict[schedule_id]['credited_hours']
            pairing['schedule_vacations'] = credited_dict[schedule_id]['vacations']
        else:
            pairing['schedule_cost'] = 0.0
            pairing['schedule_hours'] = 0.0
            pairing['schedule_vacations'] = 0
    return pairings

# -------------------------------
# 3. Pairing features
# -------------------------------

def pairing_to_features(pairing, leg_dict, schedule_dict):
    duties = pairing.get('duties', [])
    valid_duties = []
    for d in duties:
        clean_id = d.replace('PAL_LEG_', 'LEG_').replace('TDH_LEG_', 'LEG_').replace('TDH_AGR_', 'LEG_')
        if clean_id in leg_dict:
            valid_duties.append(clean_id)
    if not valid_duties:
        return {f: 0 for f in [
            'n_duties','n_deadheads','total_duration','max_duration','min_duration',
            'avg_gap','max_gap','multi_day_flag','first_hour','last_hour',
            'base','schedule_hours','schedule_cost','schedule_vacations'
        ]}
    durations = [leg_dict[d]['duration_min'] for d in valid_duties]
    n_duties = len(valid_duties)
    n_deadheads = sum(1 for d in duties if d.startswith('PAL_') or d.startswith('TDH_'))
    total_duration = sum(durations)
    max_duration = max(durations)
    min_duration = min(durations)
    sorted_duties = sorted([leg_dict[d] for d in valid_duties], key=lambda x: x['dep_dt'])
    gaps = [(sorted_duties[i+1]['dep_dt'] - sorted_duties[i]['arr_dt']).total_seconds()/60
            for i in range(len(sorted_duties)-1)]
    gaps = [max(0,g) for g in gaps]
    avg_gap = np.mean(gaps) if gaps else 0
    max_gap = np.max(gaps) if gaps else 0
    multi_day_flag = 1 if len(set(d['dep_dt'].date() for d in sorted_duties)) > 1 else 0
    first_hour = sorted_duties[0]['dep_dt'].hour
    last_hour = sorted_duties[-1]['arr_dt'].hour
    schedule_id = pairing.get('schedule')
    schedule_info = schedule_dict.get(schedule_id, {}) if schedule_id else {}
    features = {
        'n_duties': n_duties,
        'n_deadheads': n_deadheads,
        'total_duration': total_duration,
        'max_duration': max_duration,
        'min_duration': min_duration,
        'avg_gap': avg_gap,
        'max_gap': max_gap,
        'multi_day_flag': multi_day_flag,
        'first_hour': first_hour,
        'last_hour': last_hour,
        'base': pairing.get('base',0),
        'schedule_hours': schedule_info.get('n_activities',0),
        'schedule_cost': pairing.get('schedule_cost',0),
        'schedule_vacations': schedule_info.get('n_vacations',0)
    }
    return features

def build_feature_dataframe(pairings, leg_dict, schedule_dict):
    feature_rows = []
    targets = []
    groups = []
    for pairing in pairings:
        feat = pairing_to_features(pairing, leg_dict, schedule_dict)
        feature_rows.append(feat)
        targets.append(pairing.get('cost',0))
        groups.append(pairing.get('solution_id',0))
    X = pd.DataFrame(feature_rows)
    y = np.array(targets)
    groups = np.array(groups)
    print(f"Built feature matrix: {X.shape[0]} samples, {X.shape[1]} features")
    return X, y, groups

# -------------------------------
# 4. Train and predict
# -------------------------------

def train_and_cv(X, y, groups, degree=2, alpha=1.0):
    numeric_cols = ['n_duties','n_deadheads','total_duration','max_duration','min_duration',
                    'avg_gap','max_gap','first_hour','last_hour','schedule_hours',
                    'schedule_cost','schedule_vacations']
    categorical_cols = ['base','multi_day_flag']
    poly_transformer = PolynomialFeatures(degree=degree, include_bias=False)
    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    numeric_data_poly = poly_transformer.fit_transform(X[numeric_cols].values)
    categorical_data = ohe.fit_transform(X[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])
    logo = LeaveOneGroupOut()
    n_splits = logo.get_n_splits(X_final, y, groups)
    if n_splits > 1:
        for train_idx, test_idx in logo.split(X_final, y, groups):
            X_train, X_test = X_final[train_idx], X_final[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            model = Ridge(alpha=alpha)
            model.fit(X_train, y_train)
    final_model = Ridge(alpha=alpha)
    final_model.fit(X_final, y)
    return final_model, poly_transformer, ohe, numeric_cols, categorical_cols

def predict_costs(model, poly_transformer, ohe, numeric_cols, categorical_cols,
                  pairings, leg_dict, schedule_dict):
    X_new = pd.DataFrame([pairing_to_features(p, leg_dict, schedule_dict) for p in pairings])
    numeric_data_poly = poly_transformer.transform(X_new[numeric_cols].values)
    categorical_data = ohe.transform(X_new[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])
    y_pred = model.predict(X_final)
    for p, pred in zip(pairings, y_pred):
        p['pred_cost'] = pred
    return pairings



# -------------------------------
# 5. Main
# -------------------------------

def main(day_files_folder="/content/sample_data",
         schedules_file="/content/sample_data/solution_0.txt",
         pairings_file="/content/sample_data/initialSolution.txt",
         credited_hours_file="/content/sample_data/creditedHours.txt"):

    print("="*60)
    print("Crew Pairing Cost Prediction System")
    print("="*60)

    leg_dict = parse_leg_files(day_files_folder)
    schedule_dict = parse_schedules_from_file(schedules_file)
    credited_dict = load_credited_hours(credited_hours_file)

    # Attach credited hours / schedule_cost to schedules and pairings
    attach_schedule_info_to_pairings([{'schedule':k} for k in schedule_dict.keys()], credited_dict)

    # Load pairings
    pairings = parse_pairings_from_file(pairings_file, schedule_dict)
    attach_schedule_info_to_pairings(pairings, credited_dict)

    X, y, groups = build_feature_dataframe(pairings, leg_dict, schedule_dict)
    model, poly, ohe, num_cols, cat_cols = train_and_cv(X, y, groups)
    pairings_with_pred = predict_costs(model, poly, ohe, num_cols, cat_cols, pairings, leg_dict, schedule_dict)
    return {'model': model, 'pairings': pairings_with_pred, 'leg_dict': leg_dict, 'schedule_dict': schedule_dict}

if __name__ == "__main__":
    results = main(
        day_files_folder="/content/sample_data/instance1/",
        schedules_file="/content/sample_data/solution_0.txt",
        pairings_file="/content/sample_data/initialSolution.txt",
        credited_hours_file="/content/sample_data/creditedHours.txt"
    )

        # View the feature matrix
    print("\n" + "="*60)
    print("FEATURE MATRIX")
    print("="*60)

    # Extract the pairings and create a display DataFrame
    pairings = results['pairings']

    # Create feature matrix display
    feature_data = []
    for p in pairings:
        feature_data.append({
            'pairing_id': p['id'],
            'base': p['base'],
            'n_duties': len(p['duties']),
            'predicted_cost': p.get('pred_cost', 0)
        })

    feature_df = pd.DataFrame(feature_data)

    # Display first 20 rows
    print("\nFirst 20 pairings:")
    print(feature_df.head(20))

    # Display summary statistics
    print("\nSummary Statistics:")
    print(feature_df.describe())

    # Display cost distribution
    print(f"\nPredicted Cost Range:")
    print(f"  Min:  {feature_df['predicted_cost'].min():.2f}")
    print(f"  Max:  {feature_df['predicted_cost'].max():.2f}")
    print(f"  Mean: {feature_df['predicted_cost'].mean():.2f}")
    print(f"  Std:  {feature_df['predicted_cost'].std():.2f}")

    # Group by base
    print("\nBy Base:")
    print(feature_df.groupby('base')['predicted_cost'].agg(['count', 'mean', 'std']))









#!/usr/bin/env python3
"""
pairing_cost_predictor.py
Predicts crew pairing costs using Ridge regression with polynomial features.

Usage:
    python pairing_cost_predictor.py

Or in Colab/Jupyter:
    from pairing_cost_predictor import main
    results = main(
        day_files_folder="/content/sample_data",
        schedules_file="/content/sample_data/schedules.txt",
        pairings_file="/content/sample_data/pairings.txt"
    )
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
from sklearn.linear_model import Ridge
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import re
import warnings
warnings.filterwarnings('ignore')

# -------------------------------
# 1. Parse leg data from day files
# -------------------------------

def parse_leg_files(day_files_folder):
    leg_dict = {}
    files_loaded = 0
    error_rows = 0

    REQUIRED_COLS = {
        "leg_nb", "airport_dep", "date_dep", "hour_dep",
        "airport_arr", "date_arr", "hour_arr"
    }

    print(f"Loading day files from: {day_files_folder}")

    for day_num in range(1, 32):
        filepath = os.path.join(day_files_folder, f"day_{day_num}.csv")
        if not os.path.exists(filepath):
            print(f"  Warning: {filepath} not found, skipping.")
            continue

        try:
            # Auto-detect delimiter (tab or comma)
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline()
            delimiter = '\t' if '\t' in first_line else ','

            df = pd.read_csv(
                filepath,
                sep=delimiter,
                skipinitialspace=True,
                dtype=str
            )

            # Normalize column names
            df.columns = [c.strip().lstrip("#").lower() for c in df.columns]

            missing = REQUIRED_COLS - set(df.columns)
            if missing:
                raise ValueError(f"{filepath} missing required columns: {missing}")

            for _, row in df.iterrows():
                try:
                    # Flexible datetime parsing
                    dep_str = f"{row['date_dep'].strip()} {row['hour_dep'].strip()}"
                    arr_str = f"{row['date_arr'].strip()} {row['hour_arr'].strip()}"
                    dep_dt = pd.to_datetime(dep_str, errors='coerce', dayfirst=False)
                    arr_dt = pd.to_datetime(arr_str, errors='coerce', dayfirst=False)

                    if pd.isna(dep_dt) or pd.isna(arr_dt):
                        raise ValueError(f"Could not parse dep/arr datetime: {dep_str} / {arr_str}")

                    # Handle overnight flights
                    if arr_dt < dep_dt:
                        arr_dt += pd.Timedelta(days=1)

                    duration_min = (arr_dt - dep_dt).total_seconds() / 60

                    leg_id = row["leg_nb"].strip()
                    leg_dict[leg_id] = {
                        "dep_airport": row["airport_dep"].strip(),
                        "arr_airport": row["airport_arr"].strip(),
                        "dep_dt": dep_dt,
                        "arr_dt": arr_dt,
                        "duration_min": duration_min,
                    }

                except Exception as e:
                    error_rows += 1
                    print(f"  Bad row in {filepath}, leg={row.get('leg_nb','?')}: {e}")

            files_loaded += 1

        except Exception as e:
            print(f"  Error reading {filepath}: {e}")

    print(f"Loaded {files_loaded} day files | {len(leg_dict)} legs | {error_rows} bad rows")

    if not leg_dict:
        raise RuntimeError("No leg data loaded — check day files.")

    return leg_dict
# -------------------------------
# 2. Parse schedule data from text file
# -------------------------------

def parse_schedules_from_file(schedules_file):
    """
    Parse schedule data from text file.
    Format: schedule X EMPYYY (BASEZ) : LEG_XX_YY--->...

    Args:
        schedules_file: Path to schedules.txt file

    Returns:
        dict: {schedule_id: {base, employee, activities, ...}}
    """
    schedule_dict = {}

    if not os.path.exists(schedules_file):
        print(f"Warning: Schedule file {schedules_file} not found. Creating empty dict.")
        return {}

    print(f"Loading schedules from: {schedules_file}")

    try:
        with open(schedules_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Match pattern: schedule N EMPXXX (BASEN) : activities
        pattern = r'schedule\s+(\d+)\s+(\w+)\s+\((\w+)\)\s*:\s*([^;]+)'
        matches = re.findall(pattern, content)

        for schedule_num, emp_id, base, activities in matches:
            schedule_id = f"schedule_{schedule_num}"
            activity_list = [act.strip() for act in activities.split('--->')]

            schedule_dict[schedule_id] = {
                'schedule_num': int(schedule_num),
                'employee': emp_id,
                'base': base,
                'activities': activity_list,
                'n_activities': len(activity_list),
                'n_vacations': sum(1 for act in activity_list if 'VACATION' in act)
            }

        print(f"Loaded {len(schedule_dict)} schedules")
        return schedule_dict

    except Exception as e:
        print(f"Error parsing schedules: {e}")
        return {}

# -------------------------------
# 3. Parse pairing data from text file
# -------------------------------

def parse_pairings_from_file(pairings_file, schedule_dict=None):
    """
    Parse pairing data from text file.
    Format: Pairing N : Base BASEN : LEG_XX_YY , LEG_XX_YY , ...

    Args:
        pairings_file: Path to pairings.txt file
        schedule_dict: Optional schedule dict to link pairings to schedules

    Returns:
        list: List of pairing dicts
    """
    pairings = []

    if not os.path.exists(pairings_file):
        print(f"Error: Pairing file {pairings_file} not found.")
        return []

    print(f"Loading pairings from: {pairings_file}")

    try:
        with open(pairings_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Match pattern: Pairing N : Base BASEN : legs
        pattern = r'Pairing\s+(\d+)\s*:\s*Base\s+(\w+)\s*:\s*([^;]+)'
        matches = re.findall(pattern, content)

        for pairing_num, base, legs_str in matches:
            # Extract leg IDs
            leg_ids = [leg.strip() for leg in legs_str.split(',')]

            pairing = {
                'id': int(pairing_num),
                'pairing_id': f"pairing_{pairing_num}",
                'base': base,
                'duties': leg_ids,
                'n_duties': len(leg_ids),
                'cost': 0,  # Will be set if we have actual cost data
                'schedule': None,  # Will be linked if possible
                'solution_id': 1  # Default solution ID for CV
            }

            pairings.append(pairing)

        print(f"Loaded {len(pairings)} pairings")

        # Try to link pairings to schedules if schedule_dict provided
        if schedule_dict:
            link_pairings_to_schedules(pairings, schedule_dict)

        return pairings

    except Exception as e:
        print(f"Error parsing pairings: {e}")
        return []

def link_pairings_to_schedules(pairings, schedule_dict):
    """Try to link pairings to schedules by matching leg sequences."""
    for pairing in pairings:
        pairing_legs = set(pairing['duties'])

        for schedule_id, schedule_data in schedule_dict.items():
            schedule_legs = set([act for act in schedule_data['activities']
                                if act.startswith('LEG_') or act.startswith('PAL_LEG_') or act.startswith('TDH_')])

            # If pairing legs are subset of schedule legs, link them
            if pairing_legs.issubset(schedule_legs):
                pairing['schedule'] = schedule_id
                break

# -------------------------------
# 4. Extract pairing-level features
# -------------------------------

def pairing_to_features(pairing, leg_dict, schedule_dict):
    """Convert a pairing dict into feature vector."""
    duties = pairing['duties']

    # Filter valid duties that exist in leg_dict
    valid_duties = []
    for d in duties:
        # Clean leg ID (remove PAL_, TDH_AGR_, TDH_LEG_ prefixes)
        clean_id = d.replace('PAL_LEG_', 'LEG_').replace('TDH_LEG_', 'LEG_').replace('TDH_AGR_', 'LEG_')
        if clean_id in leg_dict:
            valid_duties.append(clean_id)

    if not valid_duties:
        # Return default features if no valid duties
        return {
            'n_duties': len(duties),
            'n_deadheads': 0,
            'total_duration': 0,
            'max_duration': 0,
            'min_duration': 0,
            'avg_gap': 0,
            'max_gap': 0,
            'multi_day_flag': 0,
            'first_hour': 0,
            'last_hour': 0,
            'base': pairing['base'],
            'schedule_hours': 0,
            'schedule_cost': 0,
            'schedule_vacations': 0
        }

    # Duty-level features
    durations = [leg_dict[d]['duration_min'] for d in valid_duties]
    n_duties = len(valid_duties)
    n_deadheads = sum(1 for d in duties if d.startswith('PAL_') or d.startswith('TDH_'))
    total_duration = sum(durations)
    max_duration = max(durations)
    min_duration = min(durations)

    # Compute gaps between consecutive duties
    sorted_duties = sorted([leg_dict[d] for d in valid_duties], key=lambda x: x['dep_dt'])
    gaps = []
    for i in range(len(sorted_duties)-1):
        gap = (sorted_duties[i+1]['dep_dt'] - sorted_duties[i]['arr_dt']).total_seconds() / 60
        gaps.append(max(0, gap))  # Don't allow negative gaps

    avg_gap = np.mean(gaps) if gaps else 0
    max_gap = np.max(gaps) if gaps else 0

    # Multi-day flag
    dates = [d['dep_dt'].date() for d in sorted_duties]
    multi_day_flag = 1 if len(set(dates)) > 1 else 0

    # Time features
    first_hour = sorted_duties[0]['dep_dt'].hour if sorted_duties else 0
    last_hour = sorted_duties[-1]['arr_dt'].hour if sorted_duties else 0

    # Schedule-level features
    schedule_id = pairing.get('schedule')
    schedule_info = schedule_dict.get(schedule_id, {}) if schedule_id else {}
    schedule_hours = schedule_info.get('n_activities', 0)
    schedule_cost = 0  # Placeholder
    schedule_vacations = schedule_info.get('n_vacations', 0)

    features = {
        'n_duties': n_duties,
        'n_deadheads': n_deadheads,
        'total_duration': total_duration,
        'max_duration': max_duration,
        'min_duration': min_duration,
        'avg_gap': avg_gap,
        'max_gap': max_gap,
        'multi_day_flag': multi_day_flag,
        'first_hour': first_hour,
        'last_hour': last_hour,
        'base': pairing['base'],
        'schedule_hours': schedule_hours,
        'schedule_cost': schedule_cost,
        'schedule_vacations': schedule_vacations
    }
    return features

# -------------------------------
# 5. Build DataFrame for regression
# -------------------------------

def build_feature_dataframe(pairings, leg_dict, schedule_dict):
    """Build feature matrix from pairings."""
    feature_rows = []
    targets = []
    groups = []

    print("Building features...")
    for pairing in pairings:
        try:
            feat = pairing_to_features(pairing, leg_dict, schedule_dict)
            feature_rows.append(feat)
            targets.append(pairing.get('cost', 0))
            groups.append(pairing.get('solution_id', 0))
        except Exception as e:
            print(f"  Warning: Error processing pairing {pairing.get('id', 'unknown')}: {e}")
            continue

    X = pd.DataFrame(feature_rows)
    y = np.array(targets)
    groups = np.array(groups)

    print(f"Built feature matrix: {X.shape[0]} samples, {X.shape[1]} features")
    return X, y, groups

# -------------------------------
# 6. Train model with cross-validation
# -------------------------------

def train_and_cv(X, y, groups, degree=2, alpha=1.0):
    """Train Ridge regression with polynomial features."""
    numeric_cols = ['n_duties', 'n_deadheads', 'total_duration', 'max_duration', 'min_duration',
                    'avg_gap', 'max_gap', 'first_hour', 'last_hour', 'schedule_hours',
                    'schedule_cost', 'schedule_vacations']
    categorical_cols = ['base', 'multi_day_flag']

    print(f"\nTraining model (degree={degree}, alpha={alpha})...")

    # Pipeline for numeric features
    poly_transformer = PolynomialFeatures(degree=degree, include_bias=False)

    # One-hot encoding for categorical features
    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

    # Transform features
    numeric_data = X[numeric_cols].values
    numeric_data_poly = poly_transformer.fit_transform(numeric_data)
    categorical_data = ohe.fit_transform(X[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])

    # Leave-one-solution-out CV
    logo = LeaveOneGroupOut()
    n_splits = logo.get_n_splits(X_final, y, groups)

    if n_splits > 1:
        print(f"Running {n_splits}-fold cross-validation...")
        mse_list, mae_list, r2_list = [], [], []

        for fold, (train_idx, test_idx) in enumerate(logo.split(X_final, y, groups)):
            X_train, X_test = X_final[train_idx], X_final[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            model = Ridge(alpha=alpha)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            mse_list.append(mean_squared_error(y_test, y_pred))
            mae_list.append(mean_absolute_error(y_test, y_pred))
            r2_list.append(r2_score(y_test, y_pred))

        print("\nCross-validation results:")
        print(f"  MSE: {np.mean(mse_list):.2f} ± {np.std(mse_list):.2f}")
        print(f"  MAE: {np.mean(mae_list):.2f} ± {np.std(mae_list):.2f}")
        print(f"  R²:  {np.mean(r2_list):.3f} ± {np.std(r2_list):.3f}")
    else:
        print("Note: Only one group found, skipping cross-validation")

    # Fit final model on all data
    print("\nTraining final model on all data...")
    final_model = Ridge(alpha=alpha)
    final_model.fit(X_final, y)

    return final_model, poly_transformer, ohe, numeric_cols, categorical_cols

# -------------------------------
# 7. Predict costs for pairings
# -------------------------------

def predict_costs(model, poly_transformer, ohe, numeric_cols, categorical_cols,
                 pairings, leg_dict, schedule_dict):
    """Predict costs for new pairings."""
    feature_rows = [pairing_to_features(p, leg_dict, schedule_dict) for p in pairings]
    X_new = pd.DataFrame(feature_rows)

    numeric_data = X_new[numeric_cols].values
    numeric_data_poly = poly_transformer.transform(numeric_data)
    categorical_data = ohe.transform(X_new[categorical_cols])
    X_final = np.hstack([numeric_data_poly, categorical_data])

    y_pred = model.predict(X_final)

    for p, pred in zip(pairings, y_pred):
        p['pred_cost'] = pred

    return pairings

# -------------------------------
# 8. Main execution
# -------------------------------

def main(day_files_folder="/content/sample_data",
         schedules_file="/content/sample_data/schedules.txt",
         pairings_file="/content/sample_data/pairings.txt"):
    """
    Main execution flow.

    Args:
        day_files_folder: Path to folder containing day_1.csv through day_31.csv
        schedules_file: Path to schedules.txt
        pairings_file: Path to pairings.txt

    Returns:
        dict: Contains model, pairings, leg_dict, schedule_dict
    """
    print("="*60)
    print("Crew Pairing Cost Prediction System")
    print("="*60)

    # Step 1: Load leg data
    print("\n[1/6] Loading leg data from CSV files...")
    try:
        leg_dict = parse_leg_files(day_files_folder)
    except Exception as e:
        print(f"Error loading leg data: {e}")
        return None

    # Step 2: Load schedule data
    print("\n[2/6] Loading schedule data...")
    schedule_dict = parse_schedules_from_file(schedules_file)

    # Step 3: Load pairing data
    print("\n[3/6] Loading pairing data...")
    pairings = parse_pairings_from_file(pairings_file, schedule_dict)

    if not pairings:
        print("Error: No pairings loaded. Cannot continue.")
        return None

    # Step 4: Build feature matrix
    print("\n[4/6] Building feature matrix...")
    X, y, groups = build_feature_dataframe(pairings, leg_dict, schedule_dict)

    if X.empty:
        print("Error: Feature matrix is empty. Cannot continue.")
        return None

    # Display feature statistics
    print("\nFeature statistics:")
    print(X.describe())

    # Step 5: Train model
    print("\n[5/6] Training predictive model...")
    model, poly, ohe, num_cols, cat_cols = train_and_cv(X, y, groups, degree=2, alpha=1.0)

    # Step 6: Make predictions
    print("\n[6/6] Making predictions...")
    pairings_with_pred = predict_costs(model, poly, ohe, num_cols, cat_cols,
                                       pairings, leg_dict, schedule_dict)

    # Display sample predictions
    print("\nSample predictions:")
    for i, p in enumerate(pairings_with_pred[:10]):
        print(f"  Pairing {p['id']:3d} ({p['base']}): {p['n_duties']} duties, "
              f"predicted cost = {p.get('pred_cost', 0):.2f}")

    print("\n" + "="*60)
    print("Processing complete!")
    print("="*60)

    return {
        'model': model,
        'transformers': (poly, ohe, num_cols, cat_cols),
        'pairings': pairings_with_pred,
        'leg_dict': leg_dict,
        'schedule_dict': schedule_dict
    }

if __name__ == "__main__":
    # Default paths for Colab
    results = main(
        day_files_folder="/content/sample_data/instance1/",
        schedules_file="/content/sample_data/instance1/solution_0.txt",
        pairings_file="/content/sample_data/instance1/initialSolution.txt"
    )
